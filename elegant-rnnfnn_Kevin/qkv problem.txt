File "C:\Users\Kevin Bislip\Desktop\DL\DL PPO2\CS4240_Deep_Learning_Project\elegant-rnnfnn_Kevin\a2c_ppo_acktr\
model.py", line 298, in forward
    attn_out = self.self_attn(x_rnn, mask=masks)
  File "C:\Program Files\Python38\lib\site-packages\torch\nn\modules\module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "C:\Users\Kevin Bislip\Desktop\DL\DL PPO2\CS4240_Deep_Learning_Project\elegant-rnnfnn_Kevin\a2c_ppo_acktr\
model.py", line 46, in forward
    batch_size, seq_length, embed_dim = x.size()
ValueError: not enough values to unpack (expected 3, got 2)


Q , K and V (B being the batch size, T the sequence length, d model the hidden dimensionality of X

batch_size, seq_length, embed_dim = x.size()
 torch.Size([8, 73])

warehouse has 3 dimensions, 73 obs, 8 workers(1 for each processes), FNN: 8 frames, RNN/IAM: 1 frame
batch size and number of processes become one dimension, 32 or 256. 
256 = 8*32*73*1(number of samples)
8(sequence length
73 observation dimension = 
1 frame

outputs 8 x number of actions
training batch of samples samples from mult

inference 8 agents 1 action per agent. During training think of it as 1 agent sample from buffer sample of experiences. 2 dimensions okay

Stack multiple frames when there's no 

128,73,
Miguel:
size = self.parameters['batch_size']*self.parameters['num_workers']
self.indices = np.arange(0, size, self.parameters['inf_seq_len'])
self.indices = tf.constant(np.reshape(self.indices, [-1, 1]),
                                   dtype=tf.int32)

For a fair comparison, and in order to ensure that both types
of memory have access to the same amount of information,
the sequence length parameter in the recurrent models (i.e.
number of time steps the network is unrolled when updating
the model) is chosen to be equal to the number of frames that
are fed into the FNN baseline


input_dim=train_anom_dataset.img_feats.shape[-1],
                                              model_dim=256,
                                              num_heads=4,
                                              num_classes=1,
                                              num_layers=4,